{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75610070",
   "metadata": {},
   "source": [
    "#### LLM(Large Language Model): 거대 언어 모델\n",
    "- 대규모 텍스트 데이터를 학습한 인공지능 언어 모델\n",
    "- pdf, 문서, 문장 생성...\n",
    "- OpenAI - GPT\n",
    "- Anthropic - Claude\n",
    "- Google - Gemini\n",
    "- Meta - Liama\n",
    "- X.Ai - Grok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8686a4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-2.21.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading jiter-0.13.0-cp314-cp314-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: sniffio in c:\\source\\pythonsource\\.venv\\Lib\\site-packages (from openai) (1.3.1)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\source\\pythonsource\\.venv\\Lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\source\\pythonsource\\.venv\\Lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\source\\pythonsource\\.venv\\Lib\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\source\\pythonsource\\.venv\\Lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.41.5-cp314-cp314-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\source\\pythonsource\\.venv\\Lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-2.21.0-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 44.9 MB/s  0:00:00\n",
      "Downloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.13.0-cp314-cp314-win_amd64.whl (204 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp314-cp314-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 25.7 MB/s  0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, tqdm, pydantic-core, jiter, httpcore, distro, anyio, annotated-types, pydantic, httpx, openai\n",
      "\n",
      "   --- ------------------------------------  1/11 [tqdm]\n",
      "   --- ------------------------------------  1/11 [tqdm]\n",
      "   --- ------------------------------------  1/11 [tqdm]\n",
      "   -------------- -------------------------  4/11 [httpcore]\n",
      "   -------------- -------------------------  4/11 [httpcore]\n",
      "   -------------- -------------------------  4/11 [httpcore]\n",
      "   ------------------ ---------------------  5/11 [distro]\n",
      "   --------------------- ------------------  6/11 [anyio]\n",
      "   --------------------- ------------------  6/11 [anyio]\n",
      "   --------------------- ------------------  6/11 [anyio]\n",
      "   --------------------- ------------------  6/11 [anyio]\n",
      "   ----------------------------- ----------  8/11 [pydantic]\n",
      "   ----------------------------- ----------  8/11 [pydantic]\n",
      "   ----------------------------- ----------  8/11 [pydantic]\n",
      "   ----------------------------- ----------  8/11 [pydantic]\n",
      "   ----------------------------- ----------  8/11 [pydantic]\n",
      "   ----------------------------- ----------  8/11 [pydantic]\n",
      "   ----------------------------- ----------  8/11 [pydantic]\n",
      "   ----------------------------- ----------  8/11 [pydantic]\n",
      "   ----------------------------- ----------  8/11 [pydantic]\n",
      "   ----------------------------- ----------  8/11 [pydantic]\n",
      "   -------------------------------- -------  9/11 [httpx]\n",
      "   -------------------------------- -------  9/11 [httpx]\n",
      "   -------------------------------- -------  9/11 [httpx]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ------------------------------------ --- 10/11 [openai]\n",
      "   ---------------------------------------- 11/11 [openai]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.12.1 distro-1.9.0 httpcore-1.0.9 httpx-0.28.1 jiter-0.13.0 openai-2.21.0 pydantic-2.12.5 pydantic-core-2.41.5 tqdm-4.67.3 typing-inspection-0.4.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install openai\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd7a8f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "# .env 를 환경변수로 설정\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf2b9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under a silver moon, a gentle unicorn hummed a lullaby into the quiet night and drifted asleep among the twinkling stars.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f886580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_01a79d1ac2475931006997b37712c88190b47b095b117ea4a7', created_at=1771549559.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-nano-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_01a79d1ac2475931006997b3779ae0819085704096bf0bfc0d', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_01a79d1ac2475931006997b37bd578819087ad08dbc51548b4', content=[ResponseOutputText(annotations=[], text='Under the silver moon, a gentle unicorn trotted through the quiet meadow, sprinkling starlight with every step as it whispered a lullaby and drifted off to sleep.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, completed_at=1771549564.0, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=17, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=503, output_tokens_details=OutputTokensDetails(reasoning_tokens=448), total_tokens=520), user=None, billing={'payer': 'developer'}, frequency_penalty=0.0, presence_penalty=0.0, store=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382e517",
   "metadata": {},
   "source": [
    "#### 프롬프트\n",
    "- 모델이 무엇을 해야 하는지 설명하는 텍스트 input\n",
    "\n",
    "#### 프롬프트 엔지니어링\n",
    "- 모델과 대화를 잘하는 방법\n",
    "- 모델이 최대한 일관되게, 내가 원하는 결과를 내도록 지시문을 설계하는 과정\n",
    "- 같은 프롬프트, 같은 모델, 같은 환경이어도 결과가 100% 동일하지 않을 수 있음\n",
    "\n",
    "-  reasoning : 모델의 내부 추론 강도 조절(얼마나 오래 생각할 것인지)\n",
    "    - low : 빠르게, 직관적으로\n",
    "    - medium : 일반적인 설명\n",
    "    - high : 복잡한 문제를 깊게 추론\n",
    "- instructions : 모델에게 지시하는 최상위 규칙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bc256ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aye, mostly optional, matey—but mind the treacherous reefs!\n",
      "\n",
      "JavaScript’s Automatic Semicolon Insertion (ASI) will usually drop the semicolons fer ye. But there be nasty edge cases where leavin’ ’em out changes the meaning or throws errors:\n",
      "\n",
      "- After return, throw, break, continue: Don’t put the value on the next line. ASI inserts a semicolon right after the keyword, makin’ ye return undefined, arrr.\n",
      "- Lines that start with (, [, /, +, -, or .: If the previous line didn’t end cleanly, the engine may think ye be continuin’ the same expression. Classic shipwrecks with IIFEs, array literals, regex vs division, unary plus/minus, and chained properties.\n",
      "- Postfix ++ and -- must be on the same line as their target.\n",
      "\n",
      "Practical chart:\n",
      "- Always-use-semicolons: safe in all waters.\n",
      "- No-semicolons: fine if ye know ASI’s tides—just start “risky” lines with a leading semicolon or avoid such starts, and never put the value of return/throw on the next line.\n",
      "\n",
      "So, aye, they be optional—until they ain’t. Choose a style and stick to it, ye scallywag.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    reasoning={\"effort\": \"low\"},\n",
    "    instructions=\"Talk like a pirate.\",\n",
    "    input=\"Are semicolons optional in JavaScript?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76a5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    input=\"파이썬에서 리스트 컴프리헨션이 뭐야?\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce9bc0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 정의: 리스트의 각 원소에 대해 표현식을 적용해 새로운 리스트를 만드는 간결한 구문. for와 if를 한 줄로 쓸 수 있음.\n",
      "\n",
      "- 기본 문법: [표현식 for 변수 in 이터러블]\n",
      "  - 예: [x*x for x in range(10)]  # 0,1,4,9,...,81\n",
      "\n",
      "- 조건 추가: [표현식 for 변수 in 이터러블 if 조건]\n",
      "  - 예: [x*x for x in range(10) if x%2 == 0]  # 짝수의 제곱\n",
      "\n",
      "- 중첩 루프: [표현식 for a in A for b in B]\n",
      "  - 예: [(i, j) for i in range(3) for j in range(3)]\n",
      "\n",
      "- 생성과 차이: 리스트 컴프리헨션은 리스트를 한꺼번에 생성합니다. (generator는 (표현식) 형태로 사용)\n",
      "\n",
      "- 예시 비교:\n",
      "  - 리스트: [x for x in range(1000000)]\n",
      "  - 제너레이터: (x for x in range(1000000))  # 메모리 절약\n",
      "\n",
      "- 장점/권고:\n",
      "  - 코드가 간결하고 읽기 좋음\n",
      "  - 다만 너무 복잡하면 가독성이 떨어질 수 있어 적절히 나누기 권장\n",
      "\n",
      "- 간단 예시 모음:\n",
      "  - 제곱 수 만들기: squares = [x*x for x in range(10)]\n",
      "  - 짝수 필터링: evens = [x for x in range(20) if x % 2 == 0]\n",
      "  - 문자 변환 예: upper = [c.upper() for c in \"hello\"]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    instructions=\"\"\"\n",
    "You are a backend APU assistant.\n",
    "- Answer concisely and accurately.\n",
    "- If unsure, say you are unsure.\n",
    "- do not hallucinate APIs or Facts.\n",
    "- Prefer bullet points over long prose.\n",
    "\"\"\",\n",
    "    input=\"파이썬에서 리스트 컴프리헨션이 뭐야?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26955e3",
   "metadata": {},
   "source": [
    "- You are a backend APU assistant. : 모델 정체성 지정\n",
    "- Answer concisely and accurately. : 짧고 핵심위주 정확\n",
    "- If unsure, say you are unsure. : 모르면 모른다\n",
    "- do not hallucinate APIs or Facts. : 환각 확률 낮추기\n",
    "- Prefer bullet points over long prose. : 목록의 형태로 보여줘라, 장문 쓰지 말아라"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f8d4f",
   "metadata": {},
   "source": [
    "#### 역할 부여\n",
    "- developer : instruction\n",
    "- user: input 메세지 대상자\n",
    "- assistant : 모델이 생성한 답변 가지고 있는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1be76096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026년 대한민국의 공휴일은 다음과 같습니다. 대체 공휴일도 포함해서 정리해 드릴게요.\n",
      "\n",
      "1. **신정**: 1월 1일 (목)\n",
      "2. **설날**: 1월 21일 (수) ~ 1월 23일 (금) (1월 24일 대체공휴일)\n",
      "3. **삼일절**: 3월 1일 (일) (3월 2일 대체공휴일)\n",
      "4. **어린이날**: 5월 5일 (화)\n",
      "5. **석가탄신일**: 5월 6일 (수)\n",
      "6. **현충일**: 6월 6일 (토) (6월 7일 대체공휴일)\n",
      "7. **광복절**: 8월 15일 (토) (8월 16일 대체공휴일)\n",
      "8. **추석**: 9월 29일 (화) ~ 10월 1일 (목) (10월 2일 대체공휴일)\n",
      "9. **개천절**: 10월 3일 (토) (10월 4일 대체공휴일)\n",
      "10. **한글날**: 10월 9일 (금)\n",
      "11. **크리스마스**: 12월 25일 (금)\n",
      "\n",
      "따라서 2026년의 공휴일은 총 14일입니다. (대체 공휴일 포함) 도움이 되셨길 바랍니다!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\" : \"developer\",\n",
    "            \"content\" : \"너는 비서야\"\n",
    "        },\n",
    "        {\n",
    "            \"role\" : \"user\",\n",
    "            \"content\" : \"대한민국의 대체 공휴일을 포함한 2026년 공휴일은 며칠이야.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63ce453e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002년 FIFA 월드컵에서 대한민국이 화제가 된 이유는 여러 가지가 있습니다:\n",
      "\n",
      "1. **역사적인 성과**: 대한민국은 월드컵 역사상 아시아 팀으로는 처음으로 4강에 진출했습니다. 이로 인해 많은 사람들에게 큰 감동을 주었습니다.\n",
      "\n",
      "2. **진출 과정**: 한국은 조별리그에서 포르투갈, 폴란드, 미국과 함께 G조에 속해 2승 1무로 16강에 진출했습니다. 이어서 이탈리아(8강)와 스페인(4강)을 상대로 한 경기를 이기며 역사적인 성과를 이루었습니다.\n",
      "\n",
      "3. **관중과 분위기**: 한국에서 개최된 대회였던 만큼, 홈 경기를 하는 느낌이 컸습니다. 많은 팬들이 경기장에 몰려 응원하며 단결된 국민적 열기를 보여주었습니다.\n",
      "\n",
      "4. **심판 논란**: 8강과 4강 경기에서의 판정이 논란이 되었죠. 특히 이탈리아와의 경기에서 두 번의 오프사이드 골이 취소된 사건과 스페인과의 경기에서의 VAR 논란 등으로 국제적으로 주목받았습니다.\n",
      "\n",
      "5. **문화적 영향**: 월드컵을 통해 한국의 문화와 축구가 세계적으로 알려지게 되었습니다. K-POP과 한국 영화 등에 대한 관심이 높아졌고, 한국에 대한 전반적인 긍정적 인식이 확산되었습니다.\n",
      "\n",
      "6. **영웅들의 탄생**: 이 대회에서 이동국, 홍명보, 박지성 등 여러 선수들이 많은 사랑을 받았고, 이들은 한국 축구의 아이콘으로 남았습니다.\n",
      "\n",
      "이러한 요소들이 함께해 대한민국은 2002 월드컵에서 많은 화제를 모았고, 축구 역사에서도 중요한 순간으로 기억되고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\" : \"user\",\n",
    "            \"content\" : \"2002 월드컵에서 가장 화제가 된 나라가 어딜까?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\" : \"assistant\",\n",
    "            \"content\" : \"대한민국이 화제가 됨\"\n",
    "        },\n",
    "        {\n",
    "            \"role\" : \"user\",\n",
    "            \"content\" : \"그나라가 화제가 된 이유를 상세하게 설명해\"\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a79f761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "  prompt={\n",
    "    \"id\": \"pmpt_6997c4ff4870819584ec3cf86c40bde408d0026916789b04\",\n",
    "    \"version\": \"4\",\n",
    "    \"variables\": {\n",
    "      \"customer_name\": \"성춘향\",\n",
    "      \"inquiry_type\": \"배송지연\",\n",
    "      \"situation\": \"주문한 상품이 분실되었습니다.\",\n",
    "      \"policy\": \"분실의 경우 환불처리 및 5% 쿠폰 제공\",\n",
    "      \"company_name\": \"오프픈마켓\"\n",
    "    }\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77bcdbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subject: 주문 분실 관련 환불 및 5% 쿠폰 안내\\n\\n성춘향님께,\\n오프픈마켓 고객지원팀입니다. 배송 중 주문하신 상품이 분실되었다는 소식에 깊은 유감을 표합니다. 이로 인해 불편을 끼쳐 드려 정말 죄송합니다.\\n\\n저희 정책에 따라 분실 시에는 환불 처리와 5% 쿠폰을 제공해 드립니다.\\n\\n- 환불: 원 결제 수단으로 처리되며, 처리 기간은 보통 3~5영업일 소요됩니다.\\n- 5% 쿠폰: 쿠폰은 고객님의 계정으로 발급되며, 차기 구매 시 적용 가능합니다. 쿠폰 발급 여부는 마이페이지나 이메일에서 확인하실 수 있습니다.\\n\\n원활한 처리를 위해 아래 정보를 부탁드립니다.\\n- 주문번호\\n- 결제 수단(카드/계좌 등)\\n\\n확인되는 즉시 환불 절차를 시작하고, 쿠폰 발급도 함께 처리하겠습니다. 다른 해결 방법을 원하시면 저희에게도 적극 말씀해 주세요. 최선을 다해 도와드리겠습니다.\\n\\n다시 한 번 불편을 드려 죄송합니다. 빠르게 해결해 드리기 위해 노력하겠습니다.\\n\\n감사합니다.\\n오프픈마켓 고객지원팀'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d55b276",
   "metadata": {},
   "source": [
    "#### Token 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e80f111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 파이썬에서 리스트를 간단히 만드는 문법입니다. for 루프와 조건을 한 줄로 표현합니다.\n",
      "- 형식: [표현식 for 변수 in 이터러블 if 조건] (if 조건은 생략 가능)\n",
      "\n",
      "예시\n",
      "- 제곱수 리스트: [x*x for x in range(10)]\n",
      "- 짝수만 뽑기: [x for x in range(20) if x % 2 == 0]\n",
      "- 문자열 변형: [s.upper() for s in ['apple','banana']]\n",
      "- 중첩 예시: [[i+j for j in range(3)] for i in range(2)]\n",
      "\n",
      "장점/주의\n",
      "- 코드가 간결해져 읽기 쉬운 경우가 많습니다.\n",
      "- 다만 너무 복잡하면 가독성이 떨어지므로 일반 for 루프가 더 나을 때도 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    instructions=\"\"\"\n",
    "You are a backend APU assistant.\n",
    "- Answer concisely and accurately.\n",
    "- If unsure, say you are unsure.\n",
    "- do not hallucinate APIs or Facts.\n",
    "- Prefer bullet points over long prose.\n",
    "\"\"\",\n",
    "    input=\"파이썬에서 리스트 컴프리헨션이 뭐야?\",\n",
    "    # max_output_tokens=100\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6fd6a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseUsage(input_tokens=32, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=751, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=783)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 확인\n",
    "# input_tokens=69\n",
    "# output_tokens=1081\n",
    "# total_tokens=1150\n",
    "response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30569424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00302\n",
      "4.4394원\n"
     ]
    }
   ],
   "source": [
    "total_bill = response.usage.input_tokens / 100000 * 0.05 + response.usage.output_tokens / 100000 * 0.4\n",
    "print(f\"{total_bill}\")\n",
    "print(f\"{total_bill * 1470}원\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9767b323",
   "metadata": {},
   "source": [
    "#### 파라메터\n",
    "- temperature =1 이 기본값\n",
    "- 기본값보다 높을 수록 창의적이고 예측하기 어려운 형태\n",
    "- 기본값도다 낮을 수록 일관적이고 논리적\n",
    "- 일반적으로 0.7~1 사이 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c303be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제목: 구름을 타고 천국에 간 물고기\n",
      "\n",
      "한 작은 시골 마을에 '구름이 유난히 흰 날'에만 등장하는 이변들이 있었다. 이 마을의 중앙에서 줄기 흐르는 깨끗한 개울은 마을 사람들에게 항상 사랑받는 장소였고, 이곳의 물고기들은 특별한 능력을 가졌다고 있었다. 바로 하늘의 구름을 노래하는 것이었다. 사람들은 물고기들이 함께 노래를 부를 때 하늘이 더욱 파랗게 빛난다고 믿었다.\n",
      "\n",
      "그러던 어느 날, 가장 작은 물고기 '반짝이'가 하늘을 날고 싶다는 꿈을 꾸었다. 반짝이는 “구름도 나와 함께 춤추고 싶어”라고 생각했다. 하지만 주변 물고기들은 그 꿈을 비웃었다. “너는 물속에서 사는 물고기야! 어찌 구름을 타겠다고!” 하지만 반짝이는 포기하지 않았다.\n",
      "\n",
      "어느 귀여운 구름이 반짝이에게 다가와 말했다. “너에게 기회를 주겠다. 강한 의지를 가지면 나를 탈 수 있어.” 그리고 구름은 반짝이에게 진짜 춤을 출 방법을 가르쳐 주었다. 바로 자신이 진정으로 원했던 일에 대해 진솔하게 노래하며 그 꿈을 믿는 것이었다.\n",
      "\n",
      "반짝이는 매일 물속에서 힘차게 노래하며 자신의 꿈을 더욱 확고히 했다. 이 소식을 들은 다른 물고기들도 반짝이를 응원하기 시작했고, 그들은 함께 노래를 불렀다. 그러자 하늘에서 구름이 점점 낮게 내려왔고, 반짝이와 그의 친구들은 감동의 순간이 찾아왔다.\n",
      "\n",
      "마침내, 반짝이는 깊은 숨을 내쉬고 상징적인 한 마디를 내뱉었다. “내가 원하는 구름이여, 나를 태워주세요!” 그러자 정말로 구름이 흩어져 반짝이를 감싸고 상공으로 슝 솟아올랐다.\n",
      "\n",
      "하늘에서 함께 춤추는 기분은 어땠을까? 시원한 바람, 반짝이는 빛, 그리고 친구들과 함께 하는 다시 찾은 꿈의 감정! 반짝이는 구름 위에서 고향인 물속을 바라보며 자신의 위대한 도전을 담심히 돌아보았다. “꿈은 나와 함께 할 수 있는 뜨거운 의지로 이뤄지는 것이로구나!”  \n",
      "\n",
      "그날 이후로 반짝이는 구름의 친구가 되었고, 그는 언제나 이 마을에서 더 많은 물고기들이 하늘을 날 수 있도록 늘 같은 노래를 부르게 했다. 마을 사람들은 그 마법 같은 순간을 잊지않으며 매년 그날을 기념했다. 할머니, 할아버지는 자손들에게 향기로운 개울가에서 그 물고기 이야기를 들려주었고, 그렇게 세대가 넘지만 물고기와 구름의 이야기는 영원히 전해졌다. \n",
      "\n",
      "꿈은 믿고 조울만 하면 누구에게나 올 수 있는 선물! 그 특별한 마을에는 항상 청명한 파란 하늘과 수천 마리 어울려 노래하는 물고기들이 있었으니, 한 번 더 구름 위에서의 춤을 꿈꾸는 모든 이들의 이름을 기억하게 된다!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=\"\"\"\n",
    "너는 재밌는 이야기를 만드는 소설가야\n",
    "\"\"\",\n",
    "    input=\"이야기를 하나 만들어줘\",\n",
    "    temperature=1.2\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
